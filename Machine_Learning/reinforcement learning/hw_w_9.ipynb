{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'mv' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf code_for_hw10* __MACOSX data .DS_Store\n",
    "!wget --quiet https://introml_oll.odl.mit.edu/cat-soop/6.036/static/homework/hw10/code_for_hw10.zip\n",
    "!unzip code_for_hw10.zip\n",
    "!mv code_for_hw10/* .\n",
    "\n",
    "import code_for_hw10 as code_for_hw10\n",
    "import mdp10 as mdp\n",
    "\n",
    "import numpy as np\n",
    "import math as m\n",
    "import random\n",
    "\n",
    "import pdb\n",
    "from dist import uniform_dist, delta_dist, mixture_dist, DDist\n",
    "from util import argmax_with_val, argmax\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp, q, eps = 0.01, max_iters = 1000):\n",
    "    # Your code here (COPY FROM HW9)\n",
    "    raise NotImplementedError('value_iteration')\n",
    "\n",
    "def value(q, s):\n",
    "    # Your code here (COPY FROM HW9)\n",
    "    return max(q.get(s, a) for a in q.actions)\n",
    "    raise NotImplementedError('value')\n",
    "\n",
    "def greedy(q, s):\n",
    "    # Your code here (COPY FROM HW9)\n",
    "    return max(q.actions, key=lambda a: q.get(s, a))\n",
    "    raise NotImplementedError('greedy')\n",
    "\n",
    "def epsilon_greedy(q, s, eps = 0.5):\n",
    "    if random.random() < eps:  # Explore: choose a random action\n",
    "        return uniform_dist(q.actions).draw()\n",
    "    else:  # Exploit: choose the greedy action\n",
    "        return greedy(q, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp, q, eps=0.01, max_iters=1000):\n",
    "    \"\"\"\n",
    "    Perform Value Iteration to compute the optimal Q-values.\n",
    "    \n",
    "    Parameters:\n",
    "    - mdp: an instance of the MDP class\n",
    "    - q: an instance of TabularQ to store Q-values\n",
    "    - eps: threshold for convergence (default: 0.01)\n",
    "    - max_iters: maximum iterations (default: 1000)\n",
    "    \n",
    "    Returns:\n",
    "    - q: the updated TabularQ object with optimal Q-values\n",
    "    \"\"\"\n",
    "    states = mdp.states\n",
    "    actions = mdp.actions\n",
    "    gamma = mdp.discount_factor\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        delta = 0  # Track max change in Q-values\n",
    "        q_new = q.copy()  # Copy Q-table to update synchronously\n",
    "        \n",
    "        for s in states:\n",
    "            if mdp.terminal(s):\n",
    "                continue  # Skip terminal states\n",
    "            \n",
    "            for a in actions:\n",
    "                # Compute expected value over next states\n",
    "                expected_value = sum(\n",
    "                    prob * q.get(next_s, a)\n",
    "                    for next_s, prob in mdp.transition_model(s, a).items()\n",
    "                )\n",
    "                \n",
    "                new_q_value = mdp.reward_fn(s, a) + gamma * expected_value\n",
    "                \n",
    "                delta = max(delta, abs(q.get(s, a) - new_q_value))  # Track change\n",
    "                q_new.set(s, a, new_q_value)  # Update new Q-value\n",
    "        \n",
    "        q = q_new  # Update Q-table\n",
    "        \n",
    "        if delta < eps:  # Convergence check\n",
    "            break\n",
    "\n",
    "    return q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DDist' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(code_for_hw10)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Test: Value Iteration\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mcode_for_hw10\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_solve_play\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Codes\\Machine Learning\\reinforcement learning\\code_for_hw10.py:255\u001b[0m, in \u001b[0;36mtest_solve_play\u001b[1;34m(d, draw, num_episodes, episode_length)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_solve_play\u001b[39m(d \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m, draw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    253\u001b[0m                     num_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, episode_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m    254\u001b[0m     game \u001b[38;5;241m=\u001b[39m No_Exit(d)\n\u001b[1;32m--> 255\u001b[0m     qf \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgame\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTabularQ\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m    257\u001b[0m         reward, _, animation \u001b[38;5;241m=\u001b[39m sim_episode(game, (episode_length \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m episode_length\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m    258\u001b[0m                                 \u001b[38;5;28;01mlambda\u001b[39;00m s: greedy(qf, s), draw\u001b[38;5;241m=\u001b[39mdraw)\n",
      "Cell \u001b[1;32mIn[8], line 30\u001b[0m, in \u001b[0;36mvalue_iteration\u001b[1;34m(mdp, q, eps, max_iters)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Skip terminal states\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m actions:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Compute expected value over next states\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     expected_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[0;32m     29\u001b[0m         prob \u001b[38;5;241m*\u001b[39m q\u001b[38;5;241m.\u001b[39mget(next_s, a)\n\u001b[1;32m---> 30\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m next_s, prob \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransition_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m()\n\u001b[0;32m     31\u001b[0m     )\n\u001b[0;32m     33\u001b[0m     new_q_value \u001b[38;5;241m=\u001b[39m mdp\u001b[38;5;241m.\u001b[39mreward_fn(s, a) \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m expected_value\n\u001b[0;32m     35\u001b[0m     delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(delta, \u001b[38;5;28mabs\u001b[39m(q\u001b[38;5;241m.\u001b[39mget(s, a) \u001b[38;5;241m-\u001b[39m new_q_value))  \u001b[38;5;66;03m# Track change\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DDist' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "mdp.value = value\n",
    "mdp.greedy = greedy\n",
    "mdp.epsilon_greedy = epsilon_greedy\n",
    "mdp.value_iteration = value_iteration\n",
    "\n",
    "importlib.reload(code_for_hw10)\n",
    "\n",
    "# Test: Value Iteration\n",
    "code_for_hw10.test_solve_play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dul01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
