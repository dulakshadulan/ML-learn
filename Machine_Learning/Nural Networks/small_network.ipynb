{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def sgd_step(self, lrate): pass  # For modules w/o weights\n",
    "\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, m, n):\n",
    "        self.m, self.n = (m, n)  # (in size, out size)\n",
    "        self.W0 = np.zeros([self.n, 1])  # (n x 1)\n",
    "        self.W = np.random.normal(0, 1.0 * m ** (-.5), [m, n])  # (m x n)\n",
    "\n",
    "    def forward(self, A):\n",
    "        self.A = A   # (m x b)  Hint: make sure you understand what b stands for\n",
    "        return None  # Your code (n x b)\n",
    "\n",
    "    def backward(self, dLdZ):  # dLdZ is (n x b), uses stored self.A\n",
    "        self.dLdW = None       # Your code\n",
    "        self.dLdW0 = None      # Your code\n",
    "        return None            # Your code: return dLdA (m x b)\n",
    "\n",
    "    def sgd_step(self, lrate):  # Gradient descent step\n",
    "        self.W = None           # Your code\n",
    "        self.W0 = None          # Your code\n",
    "\n",
    "\n",
    "class Tanh(Module):            # Layer activation\n",
    "    def forward(self, Z):\n",
    "        self.A = np.tanh(Z)\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):    # Uses stored self.A\n",
    "        return None              # Your code: return dLdZ\n",
    "\n",
    "\n",
    "class ReLU(Module):              # Layer activation\n",
    "    def forward(self, Z):\n",
    "        self.A = None            # Your code\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):    # uses stored self.A\n",
    "        return None              # Your code: return dLdZ\n",
    "\n",
    "\n",
    "class SoftMax(Module):           # Output activation\n",
    "    def forward(self, Z):\n",
    "        return None              # Your code\n",
    "\n",
    "    def backward(self, dLdZ):    # Assume that dLdZ is passed in\n",
    "        return dLdZ\n",
    "\n",
    "    def class_fun(self, Ypred):  # Return class indices\n",
    "        return None              # Your code\n",
    "\n",
    "\n",
    "class NLL(Module):       # Loss\n",
    "    def forward(self, Ypred, Y):\n",
    "        self.Ypred = Ypred\n",
    "        self.Y = Y\n",
    "        return None      # Your code\n",
    "\n",
    "    def backward(self):  # Use stored self.Ypred, self.Y\n",
    "        return None      # Your code\n",
    "\n",
    "\n",
    "class Sequential:\n",
    "    def __init__(self, modules, loss):            # List of modules, loss module\n",
    "        self.modules = modules\n",
    "        self.loss = loss\n",
    "\n",
    "    def sgd(self, X, Y, iters=100, lrate=0.005):  # Train\n",
    "        D, N = X.shape\n",
    "        for it in range(iters):\n",
    "            pass                                  # Your code\n",
    "\n",
    "    def forward(self, Xt):                        # Compute Ypred\n",
    "        for m in self.modules: Xt = m.forward(Xt)\n",
    "        return Xt\n",
    "\n",
    "    def backward(self, delta):                    # Update dLdW and dLdW0\n",
    "        # Note reversed list of modules\n",
    "        for m in self.modules[::-1]: delta = m.backward(delta)\n",
    "\n",
    "    def sgd_step(self, lrate):                    # Gradient descent step\n",
    "        for m in self.modules: m.sgd_step(lrate)\n",
    "\n",
    "    def print_accuracy(self, it, X, Y, cur_loss, every=250):\n",
    "        # Utility method to print accuracy on full dataset, should\n",
    "        # improve over time when doing SGD. Also prints current loss,\n",
    "        # which should decrease over time. Call this on each iteration\n",
    "        # of SGD!\n",
    "        if it % every == 1:\n",
    "            cf = self.modules[-1].class_fun\n",
    "            acc = np.mean(cf(self.forward(X)) == cf(Y))\n",
    "            print('Iteration =', it, '\tAcc =', acc, '\tLoss =', cur_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.54554894, 5.11102513, 2.794577  , 5.09300414],\n",
       "       [2.18536016, 2.98914578, 4.29944865, 5.12315769],\n",
       "       [4.35559753, 3.94150334, 1.89210183, 4.97182549]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(5,2,(3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, m, n):\n",
    "        self.m, self.n = (m, n)  # (in size, out size)\n",
    "        self.W0 = np.zeros([self.n, 1])  # (n x 1)\n",
    "        self.W = np.random.normal(0, 1.0 * m ** (-.5), [m, n])  # (m x n)\n",
    "\n",
    "    def forward(self, A):\n",
    "        self.A = A   # (m x b)  Hint: make sure you understand what b stands for\n",
    "        return self.W.T @self.A + self.W0 # Your code (n x b)\n",
    "\n",
    "    def backward(self, dLdZ):  # dLdZ is (n x b), uses stored self.A\n",
    "        self.dLdW = self.A@ np.transpose(dLdZ)       # Your code\n",
    "        self.dLdW0 = np.sum(dLdZ,axis=1,keepdims=True)      # Your code\n",
    "        return self.W@dLdZ            # Your code: return dLdA (m x b)\n",
    "\n",
    "    def sgd_step(self, lrate):  # Gradient descent step\n",
    "        self.W = self.W - lrate*self.dLdW           # Your code\n",
    "        self.W0 = self.W0 - lrate*self.dLdW0         # Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Module):            # Layer activation\n",
    "    def forward(self, Z):\n",
    "        self.A = np.tanh(Z)\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):    # Uses stored self.A\n",
    "        return dLdA*(1-self.A**2 )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):              # Layer activation\n",
    "    def forward(self, Z):\n",
    "        self.A = np.maximum(0,Z)          # Your code\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):    # uses stored self.A\n",
    "        return dLdA*(self.A>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax(Module):           # Output activation\n",
    "    def forward(self, Z):\n",
    "        exp = np.exp(Z - np.max(Z,axis=0 , keepdims=True ))\n",
    "        self.A = exp/np.sum(exp,axis=0,keepdims=True) \n",
    "        return self.A            # Your code\n",
    "\n",
    "    def backward(self, dLdZ):    # Assume that dLdZ is passed in\n",
    "        return dLdZ\n",
    "\n",
    "    def class_fun(self, Ypred):  # Return class indices\n",
    "        return np.argmax(Ypred,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLL(Module):       # Loss\n",
    "    def forward(self, Ypred, Y):\n",
    "        self.Ypred = Ypred\n",
    "        self.Y = Y\n",
    "        self.log_probs = np.log(Ypred)\n",
    "        self.loss = -np.mean(self.log_probs[Y,np.arange(Y.shape[0])])\n",
    "        return self.loss     # Your code\n",
    "\n",
    "    def backward(self):  # Use stored self.Ypred, self.Y\n",
    "        dLdYpred = np.zeros_like(self.Ypred)\n",
    "        b = self.Ypred.shape[0]\n",
    "\n",
    "        dLdYpred[self.Y,np.arange(b)] = -1 / self.Ypred[self.Y,np.arange(b)]\n",
    "        dLdYpred /= b \n",
    "        return dLdYpred   # Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    def __init__(self, modules, loss):            # List of modules, loss module\n",
    "        self.modules = modules\n",
    "        self.loss = loss\n",
    "\n",
    "    def sgd(self, X, Y, iters=100, lrate=0.005):  # Train\n",
    "        D, N = X.shape\n",
    "        for it in range(iters):\n",
    "            idx = np.random.randint(N)\n",
    "            Xt , Yt = X[:,idx:idx+1], Y[:, idx:idx+1]     \n",
    "            \n",
    "            Y_pred = self.forward(Xt)\n",
    "            \n",
    "            loss = self.loss.forward(Y_pred,Yt)                      # Your code\n",
    "\n",
    "            delta = self.loss.backward()\n",
    "            self.backward(delta)\n",
    "\n",
    "            self.sgd_step(lrate)\n",
    "\n",
    "            self.print_accuracy(it,X,Y,loss)\n",
    "\n",
    "    def forward(self, Xt):                        # Compute Ypred\n",
    "        for m in self.modules: Xt = m.forward(Xt)\n",
    "        return Xt\n",
    "\n",
    "    def backward(self, delta):                    # Update dLdW and dLdW0\n",
    "        # Note reversed list of modules\n",
    "        for m in self.modules[::-1]: delta = m.backward(delta)\n",
    "\n",
    "    def sgd_step(self, lrate):                    # Gradient descent step\n",
    "        for m in self.modules: m.sgd_step(lrate)\n",
    "\n",
    "    def print_accuracy(self, it, X, Y, cur_loss, every=250):\n",
    "        # Utility method to print accuracy on full dataset, should\n",
    "        # improve over time when doing SGD. Also prints current loss,\n",
    "        # which should decrease over time. Call this on each iteration\n",
    "        # of SGD!\n",
    "        if it % every == 1:\n",
    "            cf = self.modules[-1].class_fun\n",
    "            acc = np.mean(cf(self.forward(X)) == cf(Y))\n",
    "            print('Iteration =', it, '\tAcc =', acc, '\tLoss =', cur_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def sgd_step(self, lrate): pass  # For modules w/o weights\n",
    "\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, m, n):\n",
    "        self.m, self.n = (m, n)  # (in size, out size)\n",
    "        self.W0 = np.zeros([self.n, 1])  # (n x 1)\n",
    "        self.W = np.random.normal(0, 1.0 * m ** (-.5), [m, n])  # (m x n)\n",
    "\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.dot(self.W.T, A) + self.W0  # (m x n)^T (m x b) = (n x b)\n",
    "\n",
    "    def backward(self, dLdZ):  # dLdZ is (n x b), uses stored self.A\n",
    "        self.dLdW = np.dot(self.A, dLdZ.T)                  # (m x n)\n",
    "        self.dLdW0 = dLdZ.sum(axis=1).reshape((self.n, 1))  # (n x 1)\n",
    "        return np.dot(self.W, dLdZ)                         # (m x b)\n",
    "\n",
    "    def sgd_step(self, lrate):  # Gradient descent step\n",
    "        self.W -= lrate*self.dLdW\n",
    "        self.W0 -= lrate*self.dLdW0\n",
    "\n",
    "\n",
    "class Tanh(Module):            # Layer activation\n",
    "    def forward(self, Z):\n",
    "        self.A = np.tanh(Z)\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):    # Uses stored self.A\n",
    "        return dLdA * (1.0 - (self.A ** 2))\n",
    "\n",
    "\n",
    "class ReLU(Module):              # Layer activation\n",
    "    def forward(self, Z):\n",
    "        self.A = np.maximum(0, Z)\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):    # uses stored self.A\n",
    "        return dLdA * (self.A != 0)\n",
    "\n",
    "\n",
    "class SoftMax(Module):           # Output activation\n",
    "    def forward(self, Z):\n",
    "        return np.exp(Z) / np.sum(np.exp(Z), axis=0)\n",
    "\n",
    "    def backward(self, dLdZ):    # Assume that dLdZ is passed in\n",
    "        return dLdZ\n",
    "\n",
    "    def class_fun(self, Ypred):  # Return class indices\n",
    "        return np.argmax(Ypred, axis=0)\n",
    "\n",
    "\n",
    "class NLL(Module):       # Loss\n",
    "    def forward(self, Ypred, Y):\n",
    "        self.Ypred = Ypred\n",
    "        self.Y = Y\n",
    "        return float(np.sum(-Y * np.log(Ypred)))\n",
    "\n",
    "    def backward(self):  # Use stored self.Ypred, self.Y\n",
    "        return self.Ypred - self.Y\n",
    "\n",
    "\n",
    "class Sequential:\n",
    "    def __init__(self, modules, loss):            # List of modules, loss module\n",
    "        self.modules = modules\n",
    "        self.loss = loss\n",
    "\n",
    "    def sgd(self, X, Y, iters=100, lrate=0.005):  # Train\n",
    "        D, N = X.shape\n",
    "        sum_loss = 0\n",
    "        for it in range(iters):\n",
    "            i = np.random.randint(N)\n",
    "            Xt = X[:, i:i+1]\n",
    "            Yt = Y[:, i:i+1]\n",
    "            Ypred = self.forward(Xt)\n",
    "            sum_loss += self.loss.forward(Ypred, Yt)\n",
    "            err = self.loss.backward()\n",
    "            self.backward(err)\n",
    "            self.sgd_step(lrate)\n",
    "\n",
    "    def forward(self, Xt):                        # Compute Ypred\n",
    "        for m in self.modules: Xt = m.forward(Xt)\n",
    "        return Xt\n",
    "\n",
    "    def backward(self, delta):                    # Update dLdW and dLdW0\n",
    "        # Note reversed list of modules\n",
    "        for m in self.modules[::-1]: delta = m.backward(delta)\n",
    "\n",
    "    def sgd_step(self, lrate):                    # Gradient descent step\n",
    "        for m in self.modules: m.sgd_step(lrate)\n",
    "\n",
    "    def print_accuracy(self, it, X, Y, cur_loss, every=250):\n",
    "        # Utility method to print accuracy on full dataset, should\n",
    "        # improve over time when doing SGD. Also prints current loss,\n",
    "        # which should decrease over time. Call this on each iteration\n",
    "        # of SGD!\n",
    "        if it % every == 1:\n",
    "            cf = self.modules[-1].class_fun\n",
    "            acc = np.mean(cf(self.forward(X)) == cf(Y))\n",
    "            print('Iteration =', it, '\tAcc =', acc, '\tLoss =', cur_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dul01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
