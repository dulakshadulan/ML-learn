{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Implementing gradient descent\n",
    "In this section we will implement generic versions of gradient descent and apply these to the SVM objective.\n",
    "\n",
    "<b>Note: </b> If you need a refresher on gradient descent,\n",
    "you may want to reference\n",
    "<a href=\"https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week4/gradient_descent/2\">this week's notes</a>.\n",
    "\n",
    "### 6.1) Implementing Gradient Descent\n",
    "We want to find the $x$ that minimizes the value of the *objective\n",
    "function* $f(x)$, for an arbitrary scalar function $f$.  The function\n",
    "$f$ will be implemented as a Python function of one argument, that\n",
    "will be a numpy column vector.  For efficiency, we will work with\n",
    "Python functions that return not just the value of $f$ at $f(x)$ but\n",
    "also return the gradient vector at $x$, that is, $\\nabla_x f(x)$.\n",
    "\n",
    "We will now implement a generic gradient descent function, `gd`, that\n",
    "has the following input arguments:\n",
    "\n",
    "* `f`: a function whose input is an `x`, a column vector, and\n",
    "  returns a scalar.\n",
    "* `df`: a function whose input is an `x`, a column vector, and\n",
    "  returns a column vector representing the gradient of `f` at `x`.\n",
    "* `x0`: an initial value of $x$, `x0`, which is a column vector.\n",
    "* `step_size_fn`: a function that is given the iteration index (an\n",
    "  integer) and returns a step size.\n",
    "* `max_iter`: the number of iterations to perform\n",
    "\n",
    "Our function `gd` returns a tuple:\n",
    "\n",
    "* `x`: the value at the final step\n",
    "* `fs`: the list of values of `f` found during all the iterations (including `f(x0)`)\n",
    "* `xs`: the list of values of `x` found during all the iterations (including `x0`)\n",
    "\n",
    "**Hint:** This is a short function!\n",
    "\n",
    "**Hint 2:** If you do `temp_x = x` where `x` is a vector\n",
    "(numpy array), then `temp_x` is just another name for the same vector\n",
    "as `x` and changing an entry in one will change an entry in the other.\n",
    "You should either use `x.copy()` or remember to change entries back after modification.\n",
    "\n",
    "Some utilities you may find useful are included below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rv(value_list):\n",
    "    return np.array([value_list])\n",
    "\n",
    "def cv(value_list):\n",
    "    return np.transpose(rv(value_list))\n",
    "\n",
    "def f1(x):\n",
    "    return float((2 * x + 3)**2)\n",
    "\n",
    "def df1(x):\n",
    "    return 2 * 2 * (2 * x + 3)\n",
    "\n",
    "def f2(v):\n",
    "    x = float(v[0]); y = float(v[1])\n",
    "    return (x - 2.) * (x - 3.) * (x + 3.) * (x + 1.) + (x + y -1)**2\n",
    "\n",
    "def df2(v):\n",
    "    x = float(v[0]); y = float(v[1])\n",
    "    return cv([(-3. + x) * (-2. + x) * (1. + x) + \\\n",
    "               (-3. + x) * (-2. + x) * (3. + x) + \\\n",
    "               (-3. + x) * (1. + x) * (3. + x) + \\\n",
    "               (-2. + x) * (1. + x) * (3. + x) + \\\n",
    "               2 * (-1. + x + y),\n",
    "               2 * (-1. + x + y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(f, df, x0, step_size_fn, max_iter):\n",
    "    x = x0\n",
    "    fs = [f(x)]\n",
    "    xs = [x.copy()]\n",
    "    for i in range (max_iter): \n",
    "        x = x - step_size_fn(i)*df(x)\n",
    "        fs.append(f(x))\n",
    "        xs.append(x.copy())\n",
    "    return(x,fs,xs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def package_ans(gd_vals):\n",
    "    x, fs, xs = gd_vals\n",
    "    return [x.tolist(), [fs[0], fs[-1]], [xs[0].tolist(), xs[-1].tolist()]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15500\\2219215580.py:8: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return float((2 * x + 3)**2)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15500\\2219215580.py:14: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  x = float(v[0]); y = float(v[1])\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15500\\2219215580.py:18: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  x = float(v[0]); y = float(v[1])\n"
     ]
    }
   ],
   "source": [
    "# Test case 1\n",
    "ans=package_ans(gd(f1, df1, cv([0.]), lambda i: 0.1, 1000))\n",
    "\n",
    "# Test case 2\n",
    "ans=package_ans(gd(f2, df2, cv([0., 0.]), lambda i: 0.01, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2) Numerical Gradient\n",
    "Getting the analytic gradient correct for complicated functions is\n",
    "tricky.  A very handy method of verifying the analytic gradient or\n",
    "even substituting for it is to estimate the gradient at a point by\n",
    "means of *finite differences*.\n",
    "\n",
    "Assume that we are given a function $f(x)$ that takes a column vector\n",
    "as its argument and returns a scalar value.  In gradient descent, we\n",
    "will want to estimate the gradient of $f$ at a particular $x_0.$\n",
    "\n",
    "The $i^{th}$ component of $\\nabla_x f(x_0)$ can be estimated as\n",
    "$$\\frac{f(x_0+\\delta^{i}) - f(x_0-\\delta^{i})}{2\\delta}$$\n",
    "where $\\delta^{i}$ is a column vector whose $i^{th}$ coordinate is\n",
    "$\\delta$, a small constant such as 0.001, and whose other components\n",
    "are zero.\n",
    "Note that adding or subtracting $\\delta^{i}$ is the same as\n",
    "incrementing or decrementing the $i^{th}$ component of $x_0$ by\n",
    "$\\delta$, leaving the other components of $x_0$ unchanged.  Using\n",
    "these results, we can estimate the $i^{th}$ component of the gradient.\n",
    "\n",
    "For example, if $x_0 = (1,1,\\dots,1)^T$ and $\\delta = 0.01$,\n",
    "we may approximate the first component of $\\nabla_x f(x_0)$ as\n",
    "$$\\frac{f((1,1,1,\\dots)^T+(0.01,0,0,\\dots)^T) - f((1,1,1,\\dots)^T-(0.01,0,0,\\dots)^T)}{2\\cdot 0.01}.$$\n",
    "(We add the transpose so that these are column vectors.)\n",
    "**This process should be done for each dimension independently,\n",
    "and together the results of each computation are compiled to give the\n",
    "estimated gradient, which is $d$ dimensional.**\n",
    "\n",
    "Implement this as a function `num_grad` that takes as arguments the\n",
    "objective function `f` and a value of `delta`, and returns a new\n",
    "**function** that takes an `x` (a column vector of parameters) and\n",
    "returns a gradient column vector.\n",
    "\n",
    "**Note:** As in the previous part, make sure you do not modify your input vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_grand(f,delta=0.001):\n",
    "    def df(x):\n",
    "        dec = np.zeros_like(x)\n",
    "        \n",
    "        for i in range(len(x)):\n",
    "            zeros = np.zeros_like(x)\n",
    "            zeros[i] = delta\n",
    "            dec[i] = (f(x + zeros) - f(x - zeros)) / (2 * delta)\n",
    "\n",
    "        return dec\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def num_grad(f, delta=0.001):\n",
    "    def df(x):\n",
    "        grad = np.zeros_like(x)  \n",
    "\n",
    "        for i in range(len(x)):\n",
    "            delta_i = np.zeros_like(x)  \n",
    "            delta_i[i] = delta \n",
    "\n",
    "            grad[i] = (f(x + delta_i) - f(x - delta_i)) / (2 * delta) \n",
    "\n",
    "        return grad \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Applying gradient descent to SVM objective\n",
    "\n",
    "**Note:** In this section,\n",
    "you will code many individual functions, each of which depends on previous ones.\n",
    "We **strongly recommend** that you test each of the components on your own to debug.\n",
    "\n",
    "### 7.1) Calculating the SVM objective\n",
    "\n",
    "Implement the single-argument hinge function, which computes $L_h$,\n",
    "and use that to implement hinge loss for a data point and separator.\n",
    "Using the latter function, implement the SVM objective.\n",
    "Note that these functions should work for matrix/vector arguments,\n",
    "so that we can compute the objective for a whole dataset with one call.\n",
    "<pre> x is d x n, y is 1 x n, th is d x 1, th0 is 1 x 1, lam is a scalar </pre>\n",
    "\n",
    "Hint: Look at `np.where` for implementing `hinge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margins(x,y,th,th0):\n",
    "    return(y * (np.dot(th.T, x) + th0))\n",
    "\n",
    "def hinge(v):\n",
    "    return np.where(v < 1, 1 - v, 0)\n",
    "\n",
    "# x is dxn, y is 1xn, th is dx1, th0 is 1x1\n",
    "def hinge_loss(x, y, th, th0):\n",
    "    margins = y * (np.dot(th.T, x) + th0)  # Compute margins (1xn matrix)\n",
    "    return np.mean(hinge(margins))  # Compute mean hinge loss\n",
    "\n",
    "# x is dxn, y is 1xn, th is dx1, th0 is 1x1, lam is a scalar\n",
    "def svm_obj(x, y, th, th0, lam):\n",
    "    loss = hinge_loss(x, y, th, th0)  # Compute hinge loss\n",
    "    reg = lam * np.sum(th**2)  # Compute regularization term\n",
    "    return loss + reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2) Calculating the SVM gradient\n",
    "\n",
    "Define a function `svm_obj_grad` that returns the gradient of the SVM\n",
    "objective function with respect to $\\theta$ and $\\theta_0$ in a single\n",
    "column vector.  The last component of the gradient vector should be\n",
    "the partial derivative with respect to $\\theta_0$.  Look at\n",
    "`np.vstack` as a simple way of stacking two matrices/vectors\n",
    "vertically.  We have broken it down into pieces that mimic steps in\n",
    "the chain rule; this leads to code that is a bit inefficient but\n",
    "easier to write and debug.  We can worry about efficiency later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge(v):\n",
    "    return(np.where(v<1,1-v,0))\n",
    "\n",
    "# x is dxn, y is 1xn, th is dx1, th0 is 1x1\n",
    "def hinge_loss(x, y, th, th0):\n",
    "    margins = y*(np.transpose(th)@x+th0)\n",
    "    return np.mean(hinge(margins))\n",
    "\n",
    "# x is dxn, y is 1xn, th is dx1, th0 is 1x1, lam is a scalar\n",
    "def svm_obj(x, y, th, th0, lam):\n",
    "    loss = hinge_loss(x,y,th,th0)\n",
    "    reg = lam*np.sum(th**2)\n",
    "    return loss +reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.array([[1, 2, 3, 9, 10]])\n",
    "y1 = np.array([[1, 1, 1, -1, -1]])\n",
    "th1, th10 = np.array([[-0.31202807]]), np.array([[1.834     ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.10208421, 0.02574737, 0.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hinge(margins(X1,y1,th1,th10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the gradient of hinge(v) with respect to v.\n",
    "def d_hinge(v):\n",
    "    return (np.where(v<1,-1,0))\n",
    "\n",
    "# Returns the gradient of hinge_loss(x, y, th, th0) with respect to th\n",
    "def d_hinge_loss_th(x, y, th, th0):\n",
    "    margins = y * (np.dot(th.T, x) + th0)\n",
    "    indicator = d_hinge(margins)\n",
    "    return (np.mean(indicator*y*x,axis=1,keepdims=True))\n",
    "\n",
    "# Returns the gradient of hinge_loss(x, y, th, th0) with respect to th0\n",
    "def d_hinge_loss_th0(x, y, th, th0):\n",
    "    margins = y*(np.dot(th.T,x) + th0)\n",
    "    indicator = d_hinge(margins)\n",
    "    return (np.mean(indicator*y,axis=1,keepdims=True))\n",
    "\n",
    "\n",
    "# Returns the gradient of svm_obj(x, y, th, th0) with respect to th\n",
    "def d_svm_obj_th(x, y, th, th0, lam):\n",
    "    return np.sum(d_hinge_loss_th(x, y, th, th0), axis=1, keepdims=True) + 2 * lam * th\n",
    "\n",
    "\n",
    "# Returns the gradient of svm_obj(x, y, th, th0) with respect to th0\n",
    "def d_svm_obj_th0(x, y, th, th0, lam):\n",
    "    return (d_hinge_loss_th0(x, y, th, th0))\n",
    "\n",
    "# Returns the full gradient as a single vector (which includes both th, th0)\n",
    "def svm_obj_grad(X, y, th, th0, lam):\n",
    "    grad_th = d_svm_obj_th(X, y, th, th0,lam)\n",
    "    grad_th0 = d_svm_obj_th0(X, y, th, th0,lam)\n",
    "    return np.vstack((grad_th, np.sum(grad_th0, axis=1, keepdims=True)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dul01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
